\chapter{Introduction}
% \setlength\epigraphrule{0pt}
\epigraph{ 
Aspetti, signorina,\\
le dir\`{o} con due parole\\
chi son, e che faccio, \\
come vivo.  Vuole?%\footnote{Wait, mademoiselle,\backslash I shall tell you with two words \backslash who I am, what I do, \backslash and how I live.  May I?}
}{Rodolfo introduces himself to Mimi in the first act of \textit{La Boheme} by Giacomo Puccini (1858--1924).}

In this chapter I provide a description of the key concepts related to the research tackled in this thesis, in addition to the necessary technical background.
While I touch on the overall motivation for my research along the way (specially in Section~\ref{sec:goals}), the specific characterisation of the problems investigated in the thesis is left to the Introduction/Background section in each chapter. 

\section{Viral phylodynamics}

RNA viruses (e.g. HIV, Influenza, MERS-CoV, Ebola virus) are amongst the leading causes of morbidity and mortality in humans and livestock~\citep{Woolhouse2002}.
The combination of high mutation rates and low generation times means that these pathogens evolve at a time scale such that their genomes can be used to detect the effects of epidemiological and ecological events~\citep{Drummond2003,Grenfell2004,Duffy2008,Pybus2009}.
The recent years have witnessed an unprecedented increase in the availability of molecular sequences from thousands of organisms, specially from fast-evolving RNA viruses~\citep{Benson2014}.
This growth in the availability and quality of data has in turn made it possible to expand the repertoire of scientific questions that can be asked: what factors drive virus emergence within and transmission between populations~\citep{Dudas2017,Dudas2018}? How do population dynamics shape viral circulation patterns~\citep{Volz2013,Bedford2015}?

To tackle these questions it is necessary to bridge the fields of evolutionary biology and epidemiology on what is now known as ``viral phylodynamics''~\citep{Grenfell2004,Volz2013,Pybus2013}.
One commonly accepted definition of phylodynamics states that it is concerned with the ``study of how epidemiological, immunological, and evolutionary processes act and potentially interact to shape viral phylogenies''~\citep{Grenfell2004}.
As a theoretical framework, phylodynamics couples phylogeny-generating models (e.g. coalescent, birth-death, etc) and mathematical modelling to understand how population or epidemic dynamics map onto phylogenies, and how to incorporate data from several sources into a coherent inference framework~\citep{Kuhnert2011}.
However, as argued by~\cite{Hall2015} (Chapter 1, page 2), phylodynamics can also be understood as employing phylogenetic methods to obtain estimates of the ancestry between pathogen isolates and the timing of the lineage-splitting events, and then using this information to inform epidemiological inference, an approach called ``phylogenetic epidemiology'' by~\cite{Kuhnert2011}.
Another instance of phylogenetic models being used to inform epidemiological inference without reference to a unified theoretical framework is the estimation of past population dynamics using coalescent methods (see Section~\ref{sec:coalescent_priors}) which are then compared with epidemiological time series in order to gain epidemiological insight (see Figure 3 in~\cite{Bennett2009} for  an example).

Figure~\ref{fig:phylodyn}~\citep{Suchard2018} showcases the epidemiological analysis of 1610 Ebola virus (EBOV) complete genomes originally carried out by~\citep{Dudas2017}.
Using a generalised linear model (GLM) framework in conjunction with a phylogeny estimated from the genetic data, it is possible to reconstruct the spatial dynamics of EBOV as well as to study the association of several epidemiological predictors with viral spatial spread (bottom left panel).
Estimating a time-calibrated phylogeny (see below) also allows one to reconstruct the past population dynamics of the virus. 

\begin{figure}[!ht]
\begin{center}
\includegraphics[scale=0.30]{\dir/figs/phylodyn_BEAST_paper.png} 
\end{center}
 \caption[Phylodynamic analyses of the Ebola virus epidemic in West Africa with BEAST.]{\textbf{Phylodynamic analyses of the Ebola virus epidemic in West Africa with BEAST.}
 Modern phylodynamic methods can be used to obtain insight into the spatial dynamics (top left panel), ancestry (top right, see also Figure~\ref{fig:timetree}), epidemiological determinants (bottom left) and population dynamics (bottom right) of pathogens.
 Here I show the analysis of 1610 complete Ebola virus genomes sampled in West Africa.
 Reproduced with permission from~\cite{Suchard2018}; please see original publication for details.
 }
 \label{fig:phylodyn}
\end{figure}

The chief goal of~\textbf{dating methods} is to combine information contained in the genetic divergence between isolates with some source of external timing information in order to reconstruct time-calibrated phylogenies and, in turn, infer node ages (divergence times) and evolutionary rates.
Common sources of calibration information are the sampling dates in serially-sampled data sets and bounds/distributions for specific divergence events (e.g. the divergence between humans and chimpanzees), commonly used in studies where fossil information is available~\citep{Ho2009}.
A central object in phylodynamic investigations is the~\textbf{time-calibrated phylogeny}, a rooted phylogenetic tree in which the branch lengths are measured in units of calendar time (see Section~\ref{sec:tree_space} below for a mathematical description).
These objects are specially useful because they allow one to estimate the timing of epidemiologically relevant events and gain insight into the~\textit{tempo} of epidemics.

All dating methods rely on the assumption of a ``molecular clock'', that is, the assumption that mutations accumulate steadily at a particular rate through time~\citep{Welch2005}\footnote{Note that the molecular clock is not a metronome; mutations occur randomly rather than at deterministic intervals.}.
In their seminal work,~\cite{Zuckerkandl1962} dated the origins of different globins by assuming a uniform -- through time -- rate of molecular evolution among species and duplicated genes.
This basic assumption implies that one can measure the time $t$ of divergence between any two taxa as a function of the number of differences $d$ between their sequences, using a rate $\mu$ to convert between the scales of expected mutations and time.
There are a plethora of dating methods, ranging from maximum likelihood (ML)~\citep{Rambaut2000,Volz2017,Sagulenko2018} to Bayesian approaches~\citep{Drummond2006} -- see Table 1 in~\cite{Welch2005} and~\cite{Ho2014} for surveys.
A substantial amount of work has also been done on fast, approximate, regression-based approaches methods that allow treatment of large data sets (thousands of tips/samples) in reasonable time~\citep{To2015,Rambaut2016}.
These methods employ least squares to fit  a (possibly modified) linear regression model: 
\[ E[d_{i}] = \mu \left(t_i-t_\rho\right) ,\]
where $d_i$ is the divergence (distance) between node $i$ and the root $\rho$, $t_i$ is the age of the node and $t_\rho$ is the age of the root -- also called the tree's tMRCA.
In this model, $\mu$ is the slope of the regression model and the evolutionary rate. 
However, due to the inherent stochasticity  of the mutation process, it is possible for lineages isolated closer to the age of the root to have higher divergences than younger lineages, what would lead to negative estimates of $\mu$, which are not biologically plausible. 
Another limitation of regression-based methods is that the dependence between isolates is a direct violation of basic assumptions in linear regression, leading to biases in the estimation of the evolutionary rate (regression slope).
For a brief discussion of the pitfalls of regression-based methods to infer evolutionary rates, see~\cite{To2015} and~\cite{Rambaut2016}.

It is worth noting that the validity of molecular clock models has been intensely debated since the early days of its proposition~\citep{Ayala1999}.
Under a neutral (or constant selection) model, the expected number of mutational differences follows a Poisson distribution, whose mean and variance coincide.
The observation that the variance usually exceeds the mean in many real world data sets has fostered intense discussions on whether the neutral model upon which molecular clocks rest is appropriate\footnote{As pointed out by~\cite{Ho2006}, most controversies seem to stem from the failure to account for time-dependency of evolutionary rates.}.
In many situations, there is substantial variation in evolutionary rates between lineages, violating the assumption of a fixed, unique rate of evolution.
This needs to be accounted for in order to obtain correct estimates of node ages.
Several rate-smoothing methods have been developed to model the molecular clock and between-lineage variation, and models of molecular clock can be non-exhaustively divided in the following categories\citep{Ho2015}\footnote{Classification adapted from \url{https://github.com/sebastianduchene/NELSI/blob/master/README.md}.}:
\begin{itemize}
 \item Strict clock: a single rate is assumed for all the branches of a phylogeny, as discussed above~\citep{Zuckerkandl1962}.
 \item Local clock: a fixed number of strict clocks in the tree, so that the rate is constant for some lineages~\citep{Yoder2000, Drummond2010}.
 \item Autocorrelated models: rates vary gradually along a lineage and thus show some degree of correlation along the phylogeny~\citep{Thorne1998}.
 \item Uncorrelated models, where the rates for each of the branches are independently and identically distributed random variables, e.g. the uncorrelated log-normal clock~\citep{Drummond2006}.
\end{itemize}
Please consult~\cite{Kumar2005} for a rather enjoyable historical account of the evolution of molecular clocks over most of the last half of the 20th century.

In this thesis I shall concentrate on the uncorrelated ``relaxed'' clocks of~\cite{Drummond2006}, because of their flexibility.
Consider a rooted tree with $n$ taxa. 
This tree will have $N = 2n-3$ branches excluding the root, for which we will not estimate the rate.
\cite{Drummond2006} assume each branch evolves according to its own rate $r_i \in \mathbf{r} =\{r_1, r_2, \ldots, r_{N}\}$ and that the $r_i$ are i.i.d. random variables.
The first model to consider is the \textbf{uncorrelated exponential (UCED)} rates model, where
\[r_i \sim \text{exponential}(\lambda).\]
Let $f_{\mathbf{r}}(\cdot)$ the prior density of the rates.
Then
\begin{align}
 f_{\mathbf{r}}(\mathbf{r}) &= \prod_{i=1}^{N}\lambda e^{-\lambda r_i}, \\
 &= \lambda^{N} e^{-\lambda S},
\end{align}
from which one can note that the prior density depends on the rates $\mathbf{r}$ only through their sum $S = \sum_{i=1}^{N} r_i$, which makes clear the potential for identifiability problems.
See~\cite{Rannala2002} for more on the identifiability of phylogenetic models.

Next, consider a more flexible model, where the rates are drawn from a \textbf{log-normal distribution (UCLN)}:
\[r_i \sim \text{log-normal}(\mu, \sigma^2),\]
\begin{align}
 f_{\mathbf{r}}^{\prime}(\mathbf{r}) &= \prod_{i=1}^{N} \frac{1}{r_i\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(\ln r_i- \mu)^2}{2\sigma^2} \right), \\
 &= \frac{1}{P \cdot (2\pi)^{N/2} \cdot \sigma^{N}} \exp\left(-\frac{1}{2\sigma^2}\sum_{i=1}^{N}(\ln r_i- \mu)^2 \right),
 \end{align}
where $P = \prod_{i=1}^{N} r_i$, which appears identifiable at first glance.

These models are important in that they allow different lineages to evolve at rates that sometimes differ by orders of magnitude.
In a sense they capture the biological variability in evolutionary rates that is possible within a population, specially when considering fast evolving viruses.

The conjunction of time calibration and relaxed clocks allows us to estimate the age of the common ancestor of the circulating strains, as well as identify lineages that evolve faster than others.
In an epidemiological context, such information may prove useful when designing strategies to mitigate disease transmission, for instance.
Figure~\ref{fig:timetree} is meant to illustrate one of the end products of a phylodynamic analysis using BEAST~\citep{Drummond2007,Drummond2012}, with a time-calibrated phylogeny built using sequences from a fast-evolving RNA virus (Dengue virus serotype 2) and using a relaxed clock (log-normal) model.
This phylogeny was estimated from a MCMC sample of the posterior of phylogenies, meaning it carries the uncertainty inherently associated with not knowing the true phylogenetic relationship between strains,~\textit{i.e.}, phylogenetic uncertainty (see more below).

\begin{figure}[!ht]
\begin{center}
\includegraphics[scale=0.4]{\dir/figs/denv2_time_tree.pdf} 
\end{center}
 \caption[Time-calibrated phylogeny of DENV-2 strains circulating in the Americas.]{\textbf{Time-calibrated phylogeny of DENV-2 strains circulating in the Americas.}
 Phylogeny constructed using $90$~\textit{env} gene sequences from dengue virus serotype 2 (DENV-2) strains isolated in the Americas over a large time span. 
 Branches are coloured according to their  evolutionary rates (posterior mean) and the blue horizontal bars are the 95\% highest probability density intervals for the node ages. 
 }
 \label{fig:timetree}
\end{figure}

The growing mass of available data calls for the development of more realistic evolutionary models that can in turn be used to improve phylodynamic inference, while being statistically principled and computationally tractable~\citep{Pybus2013}.
\cite{Frost2015} list eight challenges in phylodynamic inference, of which one is of special importance to the work presented here: ``how can analytical approaches keep up with advances in sequencing?''
Moreover, recent efforts have also advocated for the real-time analysis of sequence data as way of obtaining early insight into epidemic dynamics and inform intervention~\citep{Quick2016,Dudas2017,Hadfield2017}.
The combination of rapid increase in data and the necessity of timely analyses leads to an increased demand for more efficient estimation methods.

A key assumption for many phylodynamic methods is that the phylogeny is a good~\textit{proxy} for the~\textbf{dependence structure} of the data, even if it is rarely framed in this way.
A good example of this statistical interplay is given in~\cite{Pybus2012}, where the authors use time-calibrated phylogenies together with the sequences sampling locations and  to obtain estimates of spatial epidemiological parameters such as the velocity of the epidemic wave front and the (spatial) diffusion coefficient for the West Nile virus (WNV) epidemic in the United States of America.
\cite{Pybus2012} employ the phylogeny estimated from full WNV genomes as a proxy for the latent (unobserved) spatial dependence between cases and then proceed to~\textbf{condition} on the phylogeny in order to infer the epidemiological parameters.
Similarly, methods such as the continuous-time Markov chain (CTMC) phylogeography approach of~\cite{Lemey2009} rely on the estimated phylogeny -- and traits observed at the tips/leaves -- to estimate migration (transition) rates between locations.
This is a key concept in phylodynamics: even if the phylogeny is but a nuisance parameter, one needs to ensure it is estimated correctly -- and uncertainty properly accounted for -- so that inferences made conditional on the underlying dependence structure are correct.
Consequently, even if one is interested solely in epidemiological parameters, properly accommodating uncertainty about the phylogeny~\textit{via} marginalisation is crucial to principled modelling.
As I will show in the following sections, Bayesian methods provide a way of accounting for uncertainty about parameters and models and incorporate information from different sources, being particularly useful in phylogenetics and phylodynamics.

\section{Bayesian/Laplacian approach to inference}
\label{sec:bayesian_inference}

This thesis is fundamentally a description of how modern statistical methods can be applied to the emerging field of phylodynamics to facilitate epidemiological inference.
As such, it needs a detailed description of the underlying approach to statistical inference adopted.
In what follows I shall present a short overview of the philosophical and historical roots of the Bayesian paradigm and then move on to present the necessary technical background.

\subsection{A philosophical (and historical) digression}

Statistics can understood under two main paradigms or schools of thought: orthodox/frequentist and Bayesian/Laplacian.
The contrasts between these schools stem from fundamental disagreements on the nature and meaning of probability in modelling and interpreting reality.
Under frequentism, probabilities are understood as long run frequencies of events while under (most versions of) Bayesianism, probabilities are understood as degrees of belief~\citep{Lindley2000}.
While the two schools differ in their interpretation of the meaning of probability, there is complete agreement in the~\textit{computation} of probabilities once a model has been established.
Moreover, in practice -- \textit{i.e.} real problem-solving as opposed to musing over toy problems -- there is substantial overlap and ``hybrid'' approaches often succeed~\citep{Kass2011}.

A fair and detailed comparison of these approaches is well beyond the scope of this chapter and thesis.
While preferring the Bayesian approach myself, I often find common -- short --  justifications of Bayesian inference to be wanting.
I will instead choose to gloss over important objections and assume the Bayesian paradigm for statistical inference without further justification.
I urge the reader to consult Chapter 11 of~\cite{Robert2007} for a grounded and well constructed defence of the Bayesian approach.
\cite{Jaynes2003} also offers a rather assertive argument in favour of Bayesianism.
For a grounded and modern defence of frequentism, please see~\cite{Mayo2011} and references therein.
Finally, for a modern and detailed account of~\textit{how} to do Bayesian statistics I recommend~\cite{Gelman2014b}.

The name ``Bayesian'' -- initially used in a derogatory manner -- stems from a paper published by Thomas Bayes (1702--1761) in 1763~\citep{Bayes1763}\footnote{The attentive reader will notice that Bayes was dead by the time the paper was published. His friend Richard Price finished the paper and read it in front of the Royal Society.} on so-called ``inverse probability'', a concept emerging directly from the definition of conditional probability.
It can however be argued that Pierre Simon Laplace (1749--1827) was really the first to apply mathematically rigorous ``inverse probability'' models to scientific problems (see e.g.~\cite{Laplace1774}\footnote{An English translation is provided in~\cite{Stigler1986}.}).
It is therefore my humble opinion that we owe as much or more to Laplace as we do to Reverend Bayes for laying out the foundations of (Bayesian) statistical thinking.
Thus a perhaps more accurate name for this approach would be Bayesian-Laplacian.
For convenience, however, I shall refer to this approach as~\textit{Bayesian} from here onwards.
 
\subsection{Bayes' rule}

Suppose one has a (probabilistic) model that describes how a random variable $\boldsymbol Y$ relates to a set of parameters $\boldsymbol\theta$ through a \textbf{likelihood} function $f(\boldsymbol Y| \boldsymbol\theta)$.
Suppose further that one observes a set of data $\boldsymbol y$.
Under the Bayesian approach, one aims to use~\textit{Bayes' rule} to construct the \textbf{posterior} distribution
\[ p(\boldsymbol\theta | \boldsymbol y) = \frac{f(\boldsymbol y| \boldsymbol\theta) \pi(\boldsymbol\theta)}{\int_{\boldsymbol\Theta}f(\boldsymbol y| \boldsymbol\theta) \pi(\boldsymbol\theta)d\boldsymbol\theta } ,\]
often written as $p(\boldsymbol\theta | \boldsymbol y) \propto f(\boldsymbol y | \boldsymbol\theta)\pi(\boldsymbol\theta)$.
This construction of the inference problem necessitates a probability measure over the parameter space, \textit{i.e.}, a \textbf{prior}, $\pi(\boldsymbol\theta)$.
In scientific applications, a common interpretation is that $\pi(\boldsymbol\theta)$ encodes our knowledge about the parameters $\boldsymbol\theta$~\textit{before} we observe the data $\boldsymbol y$ and $p(\boldsymbol\theta | \boldsymbol y)$ represents our updated beliefs/knowledge.

Two points are worth emphasising: firstly, in contrast to~\textit{maximum likelihood} and other approaches, Bayesian inference focuses on~\textit{integrating} over the likely values of the quantities of interest rather than~\textit{maximising}, that is, finding the value $\hat{\boldsymbol\theta}$ that maximises the likelihood.
Secondly, all inferences are based on the posterior; point estimates can be derived as expectations\footnote{Mathematically, expectations are integrals.} and any functionals of interest can be studied by transforming $p(\boldsymbol\theta | \boldsymbol y)$. 
The computation of posterior distributions is difficult for all but the simplest models of interest in practice.
Often, these distributions are not available analytically and must be approximated.
In Section~\ref{sec:mcmc} I discuss a popular method for obtaining such approximations, which will be the main focus of Chapter 2.

\section{Bayesian phylogenetics}
\label{sec:bayesian_phylogenetics}

While the application of Bayesian methods in phylogenetics is not without controversy~\citep{Barker2015}, this approach has enjoyed notable popularity in phylogenetics.
After the pioneering work of~\cite{Kuhner1995}, the mid 1990s to early 2000s saw a period of rapid development of Bayesian methods to solve phylogenetic problems~\citep{Rannala1996,Mau1997,Yang1997,Kuhner1998,Larget1999,Li2000,Suchard2001,Drummond2002}.
See~\cite{Huelsenbeck2001b} and~\cite{Holder2003} for reviews of those developments.

The main idea behind Bayesian phylogenetics is to compute a posterior distribution of the form:
\begin{equation}
 \label{eq:phylo_post_simple}
 p(T, \theta | D) \propto f(D| T, \theta)\pi(T, \theta)
\end{equation}
where $D$ is the observed data, usually an alignment of DNA sequences, $T$ is a phylogeny and $\theta$ represents parameters such as the evolutionary rate, transition/transversion rate and coalescent parameters (see below).
The likelihood is usually based on continuous-time Markov chain (CTMC) models of evolution~\citep{Hasegawa1985,Tavare1986} and be efficiently computed using a dynamic programming  procedure called ``Felsenstein's pruning algorithm''~\citep{Felsenstein1981}.
The joint prior $\pi(T, \theta)$ can be constructed in several ways, discussed in more detail below (Section~\ref{sec:coalescent_priors}).
This joint posterior can then be used to test evolutionary hypotheses, draw inference about population dynamics and obtain estimates of quantities of direct interest, such as the rate of substitution.
In many applications we are mainly interested in $\theta$, with $T$ being a nuisance parameter.
The main methodological  problem then is accommodating phylogenetic uncertainty -- \textit{i.e} uncertainty about the true underlying ancestry that generated the data -- by~\textit{marginalising} over the distribution of phylogenies to obtain a distribution $p(\theta | D)$.
I shall return to the point of efficient marginalisation in Section~\ref{sec:mcmc}.

In addition to providing a principled framework for accommodating uncertainty, Bayesian phylogenetics also allows for the estimation of directly interpretable  quantities, such as the posterior probability for a given clade, which are useful in assessing evolutionary hypotheses.
As mentioned above, the focus on marginalisation naturally leads to measures of uncertainty.
While authors such as~\cite{Huelsenbeck2002} have stated that Bayesian methods are also more computationally efficient\footnote{Insofar as the most popular method to approximate posterior distributions, Markov chain Monte Carlo, allows for simultaneous estimation of all quantities of interest.}, I argue that the main advantage of the Bayesian approach -- and perhaps its greatest weakness -- is the ability of the researcher to incorporate substantive expert knowledge into the analysis~\textit{via} careful construction of the prior(s).

As the field progressed and Bayesian methods became the~\textit{de facto} standard, researchers began to scrutinise the construction of prior distributions and their effects 
on inferences~\citep{Huelsenbeck2002,Yang2005,Alfaro2006}.
When little is known about the quantity of interest, researchers usually resort to ``non-informative, ignorance priors''.
\cite{Seaman2012}, for instance, alert that specifying such ''uninformative`` priors on model parameters can induce rather informative priors on quantities of interest, specially when these quantities are non-linear functions of the parameters.
A characteristic of phylogenetic models is that they tend to be rather complex and some parameters are non-linearly related.
See, for instance,~\cite{Yang1996} (and Figure 2 therein) for a discussion on the relationship between the Gamma heterogeneity parameter $\alpha$ and the transition/transversion ratio $\kappa$ under the HKY85~\citep{Hasegawa1985} model.
\cite{Rannala2012} and~\cite{Wang2014} also show that seemingly innocuous priors on the branch lengths can have rather extreme effects on the prior for tree length.

\subsection{The space of trees}
\label{sec:tree_space}

In order to understand the challenges of Bayesian phylogenetics it would be desirable to to have a more rigorous description of the parameter space of interest, that is, the space of time-calibrated phylogenies (TCP).
In this section I introduce the necessary technical background for a rigorous characterisation of phylogenetic space.
The presentation will follow~\cite{Semple2003} for the general theory and~\cite{Drummond2002} and~\cite{Gavryushkina2013} for time-trees, with minor adjustments.
In this thesis, I am concerned with time-calibrated phylogenies which are binary (bifurcating)\footnote{While time-calibrated phylogenies need not be fully resolved (bifurcating), I shall make this simplifying assumption throughout the thesis.}, rooted, fully-ranked labelled phylogenies.

A rooted binary tree $t \in \mathbb{T}$ on $n$ taxa is a graph $G(\boldsymbol V_t, \boldsymbol E_t)$ with $2n-2$ edges, $n-1$ internal nodes and $n$ leaf/external nodes, also called taxa -- making up a total of $2n-1$ nodes.
Each vertex (node) $v \in \boldsymbol V_t$ has degree $3$, except for a special \textit{root} internal node, denoted $\rho$, which has degree $2$.
The set $\boldsymbol V_t$ has a partial ordering, defined as follows: $u \preceq v$ if there is a unique simple path from the root $\rho$ to $v$ through $u$, in which case we say $u$ is an~\textit{ancestor} of $v$.
Denote $\boldsymbol I_t = \{ x:  x \preceq y, \: x,y \in \boldsymbol V_t \} \subset \boldsymbol V_t $ as the set of interior nodes of $t$.
The root node $\rho$ is then the ancestor of all nodes and the smallest element of the ordering imposed by $\preceq$.
The set $\boldsymbol C_t = \boldsymbol V_t \setminus \boldsymbol I_t$ is the set of exterior nodes (taxa) of $t$.

Let $X$ be a non-empty set of labels, $\phi : X \to \boldsymbol V_t$ be a bijective map and $h : \boldsymbol I_t \to \{1, 2, \ldots, |\boldsymbol I_t|\}$ an (injective)~\textit{ranking} function such that $u \preceq v$ implies $h(u) \leq h(v)$ for all $v,u \in \boldsymbol I_t$. 
Notice that $h(u) = h(v)$ implies either $u \equiv v $ or $u, v \in \boldsymbol C_t$. 
A~\textit{ranked rooted tree} is an object $t = (\boldsymbol V_t, \boldsymbol E_t, \rho, \phi, h),\: t \in \mathbb{F}$.
We can supplement $t$ with a set of edge (branch) lengths $\boldsymbol b = \{b_1, b_2, \ldots, b_{2n-2} \}$, $\boldsymbol b \in \boldsymbol B \subseteq \mathbb{R}_{+}^{2n-2}$, creating a \textit{fully-ranked rooted phylogeny} in the form of the object $(t, \boldsymbol b) = \tau \in \boldsymbol\Psi$.
For convenience, I will henceforth call $t$ a \textbf{topology} and $\tau$ a \textbf{phylogeny}.

It is well known that the cardinality of the space of (partially ranked) rooted topologies on $n$ taxa is $R_n = |\mathbb{T}| = n!(n-1)!/2^{n-1}$.
Here, however, we are concerned with \textit{fully ranked} phylogenies, specifically those for which the mapping $h$ is a height function that measures node ages in calendar units.
If we associate an age in calendar time with each of the $2n-1$ nodes in a rooted binary tree, these can then be ranked and then used to form a poset $\boldsymbol a = \{a_1, a_2, \ldots, a_{2n-1} \} \in \boldsymbol A \subset \mathbb{R}_+^{2n-1}$.
A convenient labelling is to make labels increase with age, such that $i > j$ implies $a_i \geq a_j$; thus, the root node will have label $2n-1$.
An edge $e_{i,j}$ with $i > j$ represents an ancestral lineage and node $k$ in $\boldsymbol I_t$ corresponds to a \textit{coalescence} event of two ancestral lineages at time $a_k$.

It is convenient to define $\boldsymbol a_L$ and $\boldsymbol a_I$ as the ages of the leaf and internal nodes, respectively.
Also denote $\boldsymbol a  = (\boldsymbol a_I , \boldsymbol a_L) \in \boldsymbol A \subset \mathbb{R}_+^{2n-1}$.
There exists a bijective mapping $D : \boldsymbol B \rightarrow \boldsymbol A$ that maps the branch lengths of a TCP to its node ages.
In many phylodynamic applications, taxa are sampled through time, leading to~\textit{serially-sampled} data sets~\citep{Drummond2002}.
Hence, here $\boldsymbol a_L$ is fixed (for any $\tau \in  \boldsymbol \Psi$) as it relates to the data collection process. 
These sampling patterns are important because they alone impose constraints on the space of phylogenies.
\cite{Gavryushkina2013} derive algorithms to count these objects and show that the number of fully ranked phylogenies exceeds $R_n$. 
In Figure~\ref{sfig:space_sizes} I show the ratio between the number of objects in the space we are interested in ($ F_n  = |\mathbb{F}|$) and $R_n$.

\begin{figure}[!ht]
  \begin{center}
  \includegraphics[scale=0.6]{\dir/figs/treespace_sizes_ratio.pdf} 
  \end{center}
\caption[Ratio of the number of ranked versus fully ranked trees (with unique sampling times).]{\textbf{Ratio of the number of ranked versus fully ranked trees (with unique sampling times)}.
I show the ratio between the number of of fully-ranked trees on $n$ taxa with $n$ unique sampling times ($F_n$) and ranked rooted trees ($R_n$).
Numbers extracted from Table 2.2 in~\cite{Drummond2015} which were computed following~\cite{Gavryushkina2013}.
}
\label{sfig:space_sizes}
\end{figure}

We are now in position to define the set of \textbf{intercoalescent intervals} (also called divergence times) as $\boldsymbol s = \{s_2, s_3, \ldots, s_n\} \in \boldsymbol S \subset {R}_+^{n-1}$, where $s_i = a_{n-i+1}-a_{n-i}$ for $i = 2, \ldots, n-1$ and $s_n = a_1$.
Notice that for trees with tips sampled through time, $\boldsymbol s$ will have to be slightly adjusted to include subintervals that correspond to the intervals between either a coalescence or sampling event.
As explained by~\cite{Drummond2015}, the (infinite) space of time-calibrated phylogenies (TCP) can be composed as $\boldsymbol\Psi = \mathbb{F} \times \mathbf{S}$, and it is this space I refer to as~\textbf{phylogenetic space} throughout the thesis.

Finally, let $k_i$ denote the number of existing lineages in the interval $[a_{i-1}, a_i]$, $\boldsymbol k = \{k_2, k_3, \ldots, k_n\} \in \boldsymbol K \subset \mathbb{N}^{n-1}$.
Defining these quantities -- intercoalescent intervals and numbers of lineages -- is important in that many prior distributions commonly used in Bayesian phylogenetics are based on coalescent processes and the measure of a phylogeny $\tau$ depends on it only through its coalescent intervals and numbers of lineages, $\boldsymbol s(\tau)$.
In Chapter 2 (section~\ref{sec:prior_maths}) I explore some properties of the posterior under coalescent priors.
For serially-sampled phylogenies, the number of coalescent/sampling events (and inter-coalescent intervals), $N_z = |\boldsymbol s| = |\boldsymbol k|$ is at least $n-1$ and at most\footnote{This latter case occurs when there are $n$ distinct sampling times for the $n$ tips and these fall exactly in between sampling dates, creating a ladder tree in the same way as the extreme example in the proof of Remark~\ref{rmk:stj_ergodic}. %TODO in Chapter 2
} $2(n-1)$.

\begin{remark}
\label{rmk:k_kprime}
 Suppose the phylogeny $\tau$ has a (fixed) tip sampling structure $\boldsymbol a_L$. 
 Consider a phylogeny $\tau^\star$ which is identical to $\tau$ except for the fact that it has contemporaneous tips,~\textit{i.e.}, $a_i = 0,\: \forall \: a_i \in \boldsymbol a^\star_L$, branch lengths being extended accordingly.
 The mapping $ (\boldsymbol k^\star, \boldsymbol a_L) \to \boldsymbol k$ is surjective non-injective.
\end{remark}
\begin{proof}
 When we have distinct sampling dates, these can be seen as extra nodes that will in turn induce a partitioning of the numbers of lineages.
 Notice that $\boldsymbol k^\star = \{ 2, 3, \ldots, n\}$ for any bifurcating $\tau$ with $N^\star_z = |\boldsymbol k^\star| = n - 1$, $\sum_{u = 2}^n  k_u^\star = n(n-1)$ and  $|\boldsymbol k| > N^\star_z, \: \forall \: n\geq3$.
\end{proof}
This small observation is useful for us to translate results that hold for SPR random walks on ultrametric trees ($\mathbb{T}$) to serially-sampled  ones ($\mathbb{F}$).

\subsubsection{SPR graph}
\label{sec:spr_stuff}

The subtree-prune-and-regraft (SPR) operation picks a node in a tree $t$, prunes the subtree below that node and regrafts the subtree at a different node in the tree, creating a new tree $t^\prime$ (Figure~\ref{fig:spr}A).
The SPR distance between two trees $x$ and $y$, $d_\text{SPR}(x, y)$, is then defined as the number of SPR operations needed to obtain $y$ from $x$ or vice-versa.
When $x$ and $y$ are rooted trees, we sometimes also call $d_\text{SPR}$ the rSPR (rooted SPR) distance -- this has computational implications, since rSPR is much easier to compute.
This distance is also biologically relevant, as it relates to \textit{horizontal gene transfer}, a major evolutionary mechanism in bacteria, for instance.

One way to represent the space of phylogenies is to construct a graph $G_n = (V_n, E_n)$ where each vertex is a tree topology and an edge exists between two vertices if they are ``neighbours'' in a particular sense.
In the SPR graph (Figure~\ref{fig:spr}B) two vertices (trees) $i$ and $j$ are connected (neighbours) iff  $d_\text{SPR}(i, j) = 1$.

\begin{figure}[!ht]
  \begin{center}
  \subfigure[Two SPR operations]{ \includegraphics[width=0.495\textwidth]{\dir/figs/two_sprs.jpg} }
  \subfigure[The SPR graph for $n=4$]{ \includegraphics[width=0.495\textwidth]{\dir/figs/spr_graph.jpg} }
  \end{center}
\caption{\textbf{Subtree prune-and-regraft}.
Panel A  illustrates the subtree prune-and-regraft (SPR) operation: to transform the first tree into the second, the subtree containing $(4,5)$ is pruned and regrafted at the ancestor of $(1,2)$.
The second SPR operation again prunes $(4,5)$ and regrafts the subtree at the ancestor of $\{2\}$.
In panel B I show the SPR graph for $n=4$.
Figures reproduced from~\cite{Whidden2017}.
}
\label{fig:spr}
\end{figure}

This graph-theoretic representation of phylogenetic space is useful in that studying random walks and percolation in the induced graph sheds light into the problem of traversing phylogenetic space.
The SPR graph has been studied in some detail.
For instance, we know that under an SPR random walk there exist Hamiltonian paths, that is, paths that visit each tree exactly once~\citep{Caceres2011}.
~\cite{Whidden2017} employ an optimal transportation approach to define the curvature of the graph in sense of Ricci-Ollivier and show that the SPR graph is flat in the limit.
Moreover they show that there might be negative curvature locally, namely for large topological rearrangements -- i.e. when two trees differ by one SPR operation that moves a large subtree.
In practice this means that there might be ``bottlenecks'' between trees that differ by large subtree, which can impede efficient traversal of the graph.
Finally,~\cite{Whidden2015} use the SPR graph representation to quantify the exploration of phylogenetic space by Markov chain Monte Carlo (MCMC) algorithms.
I rely on this representation in the proofs of some assertions throughout the thesis, specially in Chapter 2.

\subsubsection{The Billera-Holmes-Vogtmann space}

Phylogenetic space does not admit a canonical parametrisation, and hence there are a plethora of representations in the literature.
Here I will present the parametrisation put forth by~\cite{Billera2001}, since it constitutes a very useful representation for statistical applications (see~\cite{Willis2017} and~\cite{Dinh2017}) and also enjoys many  desirable theoretical properties~\citep{Steel2014,StJohn2017}.
The first step in constructing the Billera-Holmes-Vogtmann (BHV) space is to associate each possible tree topology with an~\textit{orthant} $\boldsymbol O_i,\: i = 1, 2, \ldots, F_n$, which corresponds to an embedding of the non-negative real numbers such that $\boldsymbol O_i \subset \mathbb{R}_+^{2n-2}$.
We are then prepared to construct an~\textit{orthant complex} $\boldsymbol \chi$ consisting of orthants of dimension $N = 2n-2$ being joined at the origin and indexed by the countable set $\Gamma$ such that (a) $\boldsymbol O_i \cap \boldsymbol O_j$ is a face of both orthants for all $\boldsymbol O_i,\boldsymbol O_j \in \boldsymbol
\chi$ and (b) each $x \in \boldsymbol\chi$ belongs to a finite number of orthants~\citep{Dinh2017}.
The orthants in this space are separated by a nearest neighbour interchange (NNI) operation.
An schematic representation of the BHV orthant complex space is show in Figure~\ref{fig:bhv_representation}.

\begin{figure}[!ht]
  \begin{center}
  \includegraphics[scale=0.45]{\dir/figs/MeganOwen_BHV.pdf} 
  \end{center}
\caption[Schematic representation of BHV space.]{\textbf{Schematic representation of BHV space}.
Notice how each quadrant is associated with a unique topology $t$ and how some geodesics pass through the origin.
Figure extracted from~\url{http://www.crm.umontreal.ca/CanaDAM2009/pdf/owen.pdf} and reproduced with permission.
}
\label{fig:bhv_representation}
\end{figure}

It can be shown that this construction admits a continuous geodesic metric with non-positive curvature.
For two phylogenies with the same tree topology, the BHV distance can be computed in a straightforward manner by any continuous metric in $\mathbb{R}_+^{N}$, since both trees lie in the same orthant.
When trees have different topologies, one needs to compute the total path length necessary to travel the edge of the orthants.
This intuitively accounts for the fact that phylogenies that differ in topology as well as in branch lengths are more different and hence should be more distant. 
This metric has many desirable properties, including allowing for a unique representation of a ``mean'' tree, which is useful when the goal is to summarise a set of trees, for instance.
\cite{Owen2011} propose an efficient way of finding the geodesic path between trees, but computing the BHV distance between any two trees remains a computationally hard task, and is substantially more expensive to compute when compared with the Robinson-Foulds (RF) distance~\citep{Robinson1981}, for example (see Chapter 3 for definitions of many phylogenetic metrics).

\subsubsection{Clade space}

For another useful representation of trees, one can also consider bipartitions of the leaves, called ``splits'' or ``clades''. %\footnote{Strictly speaking, splits are defined for unrooted phylogenies, while clades are used when discussing rooted phylogenies.}.
An example of a split is $\{A, B, C\} | \{ D, E\}$, where $A, B, C, D$ and $E$ are tips.
We will refer to the set of all possible splits on $n$ taxa as $\boldsymbol C$.
For $n \geq 3$ taxa, there are $|\boldsymbol C| = 2^{n-1}-1$ possible splits, and a given tree can contain at most $2n-2$ splits.
Two splits $U_1 | V_1$ and $U_2 | V_2$ are said to be \textit{compatible} iff at least one of the intersections $U_1 \cap U_2$, $U_1 \cap V_2$, $V_1 \cap U_2$ and $U_1 \cap U_2$ is empty.
Let $\Omega(\boldsymbol C)$ be the $\sigma$-algebra of of $\boldsymbol C$ and let $\boldsymbol C^\star \subset \Omega(\boldsymbol C)$ be  the space of pairwise compatible clades,~\textit{i.e.}, S $\boldsymbol x \in \boldsymbol C^\star$ iff all clades in $\boldsymbol x$ are pairwise compatible.
\begin{theorem}
\label{thm:splitstheorem}
 Splits-equivalence~\citep{Buneman1971}: if $\boldsymbol c$ is set of pairwise compatible splits, then there exists one and only one topology $t$ that corresponds to $\boldsymbol c$.
 Equivalently: there is a bijective mapping $f: \mathbb{T}  \leftrightarrow \boldsymbol C^\star$.
%  \lm{
%  I wonder if we need to add the extra condition that $\bigcup_{c_i \in \boldsymbol c} l(c_i) = l(t)$, where $l(x)$ is a function that lists the leaves/tips of a tree/clade $x$.
%  }
\end{theorem}
Notice this space is of much smaller cardinality than the space of trees (topologies), which facilitates its analysis in empirical settings (see Section~\ref{sec:cladeSwitch} in chapter 3).

Having described the parameter space of interest, I shall now move on to detail the construction of prior measures on $\boldsymbol\Psi$.

\subsection{Coalescent models as phylogenetic priors}
\label{sec:coalescent_priors}

As discussed above, performing Bayesian  inference entails assigning a probability measure over the parameter space.
One way of constructing a prior measure on the space of phylogenies is by considering the so-called~\textit{coalescent}, described in the pioneering work of~\cite{Kingman1982}.
Coalescent theory seeks to mathematically describe the joining (coalescence) of lineages backwards in time until their common ancestor.
The main idea is to relate the effective population size $N_e$ to the probability of any two lineages joining at a certain time $t$ in the past.
For instance, in a haploid population with random mating and constant (through time) size of $N_e$, there are $2N_e$ possible coalescence points (parents) in the previous generation, which leads to there being a probability of $1/2N_e$ that any two lineages coalesce.
See below for other models of population dynamics that allow for $N_e$ to change over time.
When $N_e$ is sufficiently large, we can approximate the probability that any two lineages coalesce after time $t$ by
$$ P(t) = \frac{\exp\left( -(t-1)/2N_e\right)}{2N_e}  $$

\subsubsection{Parametric tree priors}

As seen above, the simplest model one can consider is when $N_e$ has remained constant through time, at size $N$,  $N_e (t) = N$.
This population size $N$ hyper-parameter can be given a prior and estimated from the data.
This model is suitable whenever the population has remained stable over the time span of the most recent common ancestor of the samples, and provides a baseline to which more parameter-rich models can be compared.
One such example of more complex model is the exponential model, which has two parameters: the population size at present $N_0$ and the growth rate $r$.
The assumption is that population grew exponentially since the time to the most recent common ancestor (tMRCA):
$$ N_e(t) = N_0 \exp(-rt), $$
which is suitable to the analysis of early viral samples from epidemics due to initial epidemic growth being approximately exponential.

Let $\tau = (t, \boldsymbol b)$ be a phylogeny and $ \psi(\tau) = (\boldsymbol s, \boldsymbol k)$ be its inter-coalescent intervals and numbers of lineages, respectively.
Under a coalescent model with constant (fixed) population size $N_e$ then~\citep[Chapter 2]{Drummond2015}:
\begin{align}
\label{eq:coal_prior} 
\pi_0(\tau| N_e) &= \prod_{i= 1}^{2n-1} \frac{{k_i\choose 2}}{N_e}\exp\left(-\frac{{k_i\choose 2}s_i}{N_e}\right), \\
             &=  \frac{1}{(N_e)^{n-1}}\prod_{j = 2}^{2n-1}\exp\left(-k_j(k_j -1)\frac{s_j - s_{j-1}}{2N_e}\right), \\
             &\propto \exp\left(-\sum_{j=2}^{2n-1} k_j(k_j -1)(s_j - s_{j-1}) \right). \
\end{align}

\subsubsection{Non-parametric models}

These parametric models might, however, prove too inflexible in approximating population trajectories in real data.
Fortunately there are non-parametric models that allow a flexible approach to demographic modelling by constructing a piece-wise process which models population size changes between coalescent events (inter-coalescent intervals)~\citep{Pybus2000,Minin2008,Gill2012}.

The first such model I will consider here is the Skyride model~\citep{Minin2008}, which improves on previous semi-parametric models~\citep{Pybus2000} of piece-wise population size change by (i) assuming population size changes smoothly over time and (ii) places a smooth Gaussian process prior on the population sizes.
Skyride operates on inter-coalescent intervals, i.e., intervals of time between coalescent events.
For a phylogeny with $n$ tips/leaves, let $\boldsymbol s = ( \boldsymbol s_2, \ldots, \boldsymbol s_n )$ be the inter-coalescent intervals.
If sampling is heterochronous, sampling times further divide inter-coalescent intervals in sub-intervals, i.e., $\boldsymbol s_k = (s_{k0}, \ldots, s_{kj_{k}} ) $.
If we denote the population sizes by $\boldsymbol \theta = ( \theta_2, \ldots, \theta_n )$, the likelihood becomes 
$$ Pr(\boldsymbol s | \boldsymbol \theta) = \prod_{k = 2}^n Pr(s_k | \theta_k), $$
with
$$ Pr(s_k | \theta_k) = \frac{n_{k0} (n_{k0} - 1)}{2\theta_k} \exp\left( - \sum_{j=0}^{j_{k}} \frac{ n_{kj} (n_{kj} - 1)s_{kj} }{2\theta_k}\right). $$
If we make the convenient transformation $\gamma_k = \log(\theta_k), k = 2, \ldots, n $, we can then place the Gaussian Markov random field (GMRF) prior on $\boldsymbol \gamma $:
$$ Pr(\boldsymbol \gamma | \tau ) \propto \tau^{(n-2)/2} \exp\left(- \frac{\tau}{2} \sum_{k = 2}^{n-1} \frac{(\gamma_{k + 1} -\gamma_k)^2}{\delta_k} \right), $$
where $\delta_k$ is the (1d) distance between intervals and $\tau$ is the precision parameter associated with the smoothing.
For details please see~\cite{Minin2008}.

The second model I will consider is the Skygrid model, an extension of the Skyride that allows for multiple loci.
While in Skyride the estimated trajectory changes at coalescent times, in Skygrid changes occur at pre-specified fixed points in (real) time.
This allows population sizes to be estimated for multiple genealogies at once, e.g., when several genes are under analyses and have different genealogies.
The researcher must select the number $M$ of grid points to be used and a cut-off $K$ in calendar time.
The cut-off $ K $ is crucial to the Skygrid analysis, as it is the last point at which population sizes change and hence should be chosen commensurate with the age of the root.
As with Skyride, the smoothness of the Skygrid prior is controlled by a precision parameter $\tau$.
The Skygrid model presents better statistical properties and is more general, which has led to it superseding Skyride in recent years.
These models are parameter-rich and their use is preferable when the data are strongly informative about population history.
More recent developments allow for the inclusion of (time) covariates to inform the population sizes in Skygrid~\citep{Gill2016}, which not only promises to help with~\textit{understanding} population dynamics but also offers a way of introducing information to inform the non-parametric prior and regularise inference.

Historically, in addition to being used to construct prior distributions these models have been used directly to investigate how distinct patterns of coalescence and genetic diversity relate to population epidemiological processes, specially for viruses~\citep{Rodrigo1999,Pybus2000,Pybus2009}.
Examples include the analysis of the dynamics of Hepatitis C virus (HCV) in Egypt by~\cite{Pybus2003} which found that ''...the Egyptian HCV epidemic was initiated and propagated by extensive antischistosomiasis injection campaigns`` and~\cite{Rambaut2008} who analysed over $1300$ Influenza virus complete genomes and found differences in the evolutionary dynamics of the A/H3N2 and A/H1N1 subtypes by spotting differences in their skyline plots (Figure 1 therein).

\subsubsection{Other priors}

While the coalescent offers a flexible framework for modelling the population dynamics from genealogies, it also relies on restrictive assumptions.
For instance, the coalescent assumes that the fraction of sampled individuals (number of taxa, $n$) is a negligible fraction of total population~\citep{Fu2006,Volz2009}.
With the rapid increase in the number of sequences, it is quite possible that most cases in an epidemic could be sampled, therefore rendering this assumption problematic.
To address this and also allow for more explicit models of epidemic dynamics, several models based on birth-death processes,~\cite{Volz2009},~\cite{Rasmussen2011} and~\cite{Stadler2011} have developed approaches that incorporate other population models such the Yule process and epidemic models such as the Susceptible-Infected-Removed (SIR) model.
Similarly to the coalescent, these models too can be used as prior measures for phylogenies, and in addition be used to estimate parameters of interest such as the basic reproductive number, $R_0$.

\section{Markov chain Monte Carlo}
\label{sec:mcmc}

Since the phylogenetic posterior~(\ref{eq:phylo_post_simple}) is not available in closed-form even for the simplest models, it must be numerically approximated.
In the following sections I introduce the necessary mathematical background for Markov chain Monte Carlo (MCMC), which I will base mostly on~\cite{Geyer2011}.
For ease of exposition, whenever a choice is to be made between a discrete and a continuous setting I shall assume the latter -- as discussed above, phylogenetic space has both discrete and continuous components.

Suppose one aims to sample from a distribution\footnote{I will assume the target is either normalised or can be normalised,~\textit{i.e.}, $ 0 < \int_{\mathcal{X}} \pi_d(x) <\infty$. } $\pi(\cdot)$ defined on a sample space $\mathcal{X}$, with density $\pi_d$ such that $\forall U \subseteq \mathcal{X}$
\[ \pi(U) = \frac{\int_{U} \pi_d(x)dx }{\int_{\mathcal{X}} \pi_d(x)dx}. \]
One reason to obtain samples from $\pi(\cdot)$ is to compute expectations of (Borel-measurable) functionals $g :\mathcal{X} \to \mathbb{R}$ such that\footnote{It is important to note that $\pi_d$ is normalised in this setting.}
\[\mu_g := \mathbb{E}_{\pi} [g(X)] = \int_{\mathcal{X}} g(x) \pi_d(x) dx. \] 
Classic Monte Carlo theory says that, under some mild regularity conditions, if one obtains a sample of i.i.d. random variables $\boldsymbol Z = \{ Z^{(1)}, Z^{(2)}, \ldots, Z^{(N)} \} \sim \pi$, then
\[ \widehat{\mu_g} = N^{-1}\sum_{i=1}^N g(Z^{(i)}), \]
is an unbiased estimate of $\mu_g$.
Moreover, the standard deviation of $\widehat{\mu_g}$ is $\mathcal{O}(\frac{1}{\sqrt{N}})$, meaning we can make the estimate as precise as desired by increasing the number of samples $N$.
Obtaining $\boldsymbol Z$ directly, however, might be impractical.
Instead, Markov chain Monte Carlo (MCMC) is a technique to draw samples by constructing a Markov chain $\{X_i\}$ on $\mathcal{X}$ that has $\pi(\cdot)$ as its~\textit{limiting} (or stationary) distribution. 
More formally, we want to construct $\{X_i\}$ with transition probabilities $P(x, dy)$ such that
\[ \pi(dy) = \int_{\mathcal{X}} \pi(dx) P(x, dy)  \]
for all $x,y \in \mathcal{X}$.
If we are able to draw samples from this Markov chain, then for a sufficiently large number of samples we will obtain a collection of random variables that are approximately drawn from $\pi(\cdot)$.

We are still however left with the task of finding an appropriate $P(x, dy)$.
One useful simplifying assumption usually made is that $P(x, dy)$ is~\textit{reversible}:
\[ \pi(dx) P(x, dy) = \pi(dy) P(y, dx) .\]
This condition is also known as~\textbf{detailed balance} and ensures $\{X_i\}$ has $\pi(\cdot)$ as its stationary (limiting) distribution.
One of the simplest ways of constructing a reversible Markov is the so-called Metropolis-Hastings algorithm, described in Section~\ref{sec:MH}. %TODO: next section (if doesn't change).

\subsection{Metropolis-Hastings}
\label{sec:MH}

Let $q_\sigma(x, y)$ be a~\textit{candidate-generating} density with indexing parameter %\footnote{The reasons for the inclusion of this parameter shall become clearer in Section~\ref{sec:practical}.}
$\sigma$ such that $\int_{\mathcal{X}} q_\sigma(x, v) dv = 1 \: \forall x \in \mathcal{X}$. 
Now consider a Markov chain $Q_\sigma(x, \cdot)$ such that $Q_\sigma(x, y) \propto q_\sigma(x, y) dy$.
The so-called Metropolis-Hastings (MH) algorithm~\citep{Metropolis1953,Hastings1970} consists of constructing a Markov chain with acceptance probability
\begin{equation}
 \label{eq:acceptance}
 \alpha_\sigma(x, y) = \min \left[1, \frac{\pi_d(y) q_\sigma(y, x)}{\pi_d(x) q_\sigma(x, y)} \right],
\end{equation}
with $\alpha_\sigma(x, y) = 1$ if $\pi_d(x) q_\sigma(x, y) = 0$~\citep[p. 329]{Chib1995}.
The quantity $q_\sigma(y, x)/ q_\sigma(x, y)$ is called the ``Hastings ratio'' and acts as a correction factor to ensure we sample from the desired target (see below).

Computationally, MH can be described as generating samples $\boldsymbol Z$ as follows:
\begin{enumerate}
 \setcounter{enumi}{-1}
 \item Pick some $Z^{(0)} \in \mathcal{X}$ with $\pi_d(Z^{(0)}) > 0$;
 
 for $n = 0$ to $M$:
 \item Given $Z^{(n)}$, generate a \textit{proposal} $Y^{(n + 1)} \sim Q_\sigma(Z^{(n)}, \cdot)$;
 \item Sample $u \sim \text{Uniform}(0, 1)$;
 \item If $\alpha_\sigma(Z^{(n)},  Y^{(n + 1)}) > u$, set $Z^{(n + 1)} = Y^{(n + 1)}$, otherwise $Z^{(n + 1)} = Z^{(n)}$ ;
\end{enumerate}
This can be shown to correctly sample from $\pi(\cdot)$ for appropriately chosen $q_\sigma(\cdot, \cdot)$ (see below) and is straightforward to implement on a computer.
The MH algorithm is a very popular workhorse of MCMC, much due to its simplicity of implementation.
Good introductions can be found in~\cite{Chib1995} and~\cite{Robert2015}.

While the MH algorithm is by far the most popular algorithm in MCMC, it is by no means the only one.
A very popular method is the Gibbs sampler~\citep{Geman1984}, an algorithm whereby samples from $\pi(\cdot)$ are drawn from a series of conditional distributions, which can be quite useful when sampling Bayesian posterior distributions based on conjugate priors.
When sampling from spaces with varying dimension, the so-called ``Reversible-jump'' MCMC of~\cite{Green1995} has enjoyed great success.
Perhaps not surprisingly, many of these algorithms are special instances of a more general sampling procedure, as shown by~\cite{Keith2004}.
A common criticism of MH is the fact that the transitions (step 3 above) do not take into account the structure of the target, often leading to random-walk behaviour.
Algorithms such as the Metropolis-adjusted Langevin algorithm (MALA)~\citep{Roberts1998} and Hamiltonian (Hybrid) Monte Carlo (HMC)~\citep{Duane1987,Neal2011} exploit the structure of the target -- usually in the form of gradients -- to perform guided transitions and increase efficiency.
I discuss this point further in Chapter 6.

\subsection{Transition kernels}
\label{sec:kernels}

Successful exploration of the target distribution depends crucially on the choice of $Q_\sigma(\cdot, \cdot)$.
In particular, the~\textbf{transition kernel}
\begin{equation}
 \label{eq:transition_kernel}
 \kappa_\sigma(x, dy) = q_\sigma(x, y) \alpha_\sigma(x,y)dy  + \left[1 - \int_{\mathcal{X}} q_\sigma(x, y) \alpha_\sigma(x,y)dy \right]\delta_x(dy),
\end{equation}
where $\delta_x(u) = 1$ if $x \in u$ and $0$ otherwise, needs to be constructed carefully to ensure correct and efficient sampling.
In section~\ref{sec:practical} I briefly discuss some of the aspects, both practical and theoretical, involved in designing effective transition kernels.
Further discussion of transition kernels in MCMC for Bayesian phylogenetics can be found in Chapters 2 and 6.

For complex parameter spaces, the construction of the proposal might be quite complicated, and the the density  $q_\sigma(\cdot, \cdot)$ might be hard to compute.
\cite{Green2003} proposes a constructive method for computing the acceptance ratio that simplifies calculations, specially when considering varying-dimension problems.
Denote the current state of the chain by $x$ and the proposed (candidate) state by $x^\prime$.
The main idea is to sample a set of auxiliary random variables $\boldsymbol u \in \boldsymbol U$ from a distribution $g(\cdot)$ and then apply a transformation $h : \mathcal{X} \times \boldsymbol U \to \mathcal{X} $ such that $x^\prime = h(x, \boldsymbol u)$.
To obtain $x$ from $x^\prime$ the procedure is to generate a set $\boldsymbol u^\prime \in \boldsymbol U$ from a (potentially different) distribution $g^\prime(\cdot)$ and apply a transform $h^\prime : \mathcal{X} \times \boldsymbol U \to \mathcal{X}$ to get $x = h^\prime(x^\prime, \boldsymbol u^\prime)$.
The acceptance probability can then be written as 
\[
 \alpha^\star_\sigma(x, y) = min \left[1, \frac{\pi_d(y) g_d^\prime(\boldsymbol u^\prime)}{\pi_d(x) g_d(\boldsymbol u)} |J| \right],
\]
with $J = \text{det} \left[\frac{\partial (x^\prime, \boldsymbol u^\prime) }{\partial (x, \boldsymbol u)} \right]$.
See~\cite{Holder2005} for an application of this technique in Bayesian phylogenetics, where it was used to correct an error in the original derivation of the \verb|LOCAL| transition kernel~\citep{Larget1999}.


\subsection{General considerations on MCMC}
\label{sec:practical}

In practice, there are many details that determine the effectiveness of MCMC as a numerical method.
One of the key challenges in efficiently computing an approximation of the target is the boldness of the proposing mechanism: propose extreme (bold) values and the candidate is likely to be rejected, making the chain be stuck for long periods; propose conservatively and the candidate is likely to be accepted but the chain will move very little away from the current state, leading to poor exploration.
It is therefore desirable to construct the candidate generating density so as to strike the best balance between bold and conservative proposals, a problem called optimal scaling~\citep{Roberts1998,Roberts2001}.
\cite{Gelman1996} suggest that under some regularity conditions on the target $\pi(\cdot)$, an acceptance probability of $0.234$ when the dimension of $\mathcal{X}$ is large enough will produce optimal sampling when compared with independent samples from $\pi(\cdot)$. 
More modern algorithms are~\textbf{adaptive}, meaning the transition kernel indexing parameter $\sigma$ can be ``tuned'' using the previous history of the chain to achieve the optimal acceptance probability~\citep{Haario2001}.

Another issue that has received much attention in the literature is determining whether $\{X_i\}$ has~\textit{converged in distribution} to $\pi(\cdot)$.
See~\cite{Robert2004} and~\cite{Meyn2012} for the technical details and Chapter 3 for a review on (statistical) tools to assess convergence.
Related to the issue of convergence is ascertaining that the effect of the initial state $Z^{(0)}$ is negligible.
In actual practice, approximations to functionals are usually done considering only samples after a \textbf{warm-up} (burn-in) number of iterations $W$,~\textit{i.e.}, $ \hat{\pi}(f) = (M - W)^{-1}\sum_{i = W + 1}^{W + M} f(Z^{(i)})$ (see Chapter 3).
We are usually interested on~\textbf{mixing}, that is, how well the Markov chain explores the target distribution.
Since the samples obtained by MCMC are fundamentally correlated, we can measure its~\textit{efficiency} by estimating how many independent samples from $\pi(\cdot)$ were produced.
This quantity is called the~\textbf{effective sample size} (ESS) and I provide definitions and discussion in Chapter 3.
In short, the closer the ESS is to the chain length\footnote{It is technically possible to have ESS larger than $M$ when using over-relaxation techniques, but this is a fringe case.} $M$, the more efficient is MCMC in approximating $\pi(\cdot)$.
In practical applications one also has to consider computational (``wall clock'') performance,~\textit{i.e.}, how fast a particular method produces a given sample, measured for instance in ESS/hour.

In addition to efficiency it is important to ensure comprehensive exploration of the target distribution.
In order for samples obtained with MCMC to be of any practical use, they need to represent the~\textbf{typical set} of the target distribution.
Intuitively, this is to be understood as exploring the ``bulk'' of the distribution, where most of its mass lies.
More formally, the typical set can be defined as $\boldsymbol S_\epsilon \in \mathcal{X}$ such that for all sequences $\boldsymbol X = \{X_1, X_2, \ldots, X_n \} \in \boldsymbol S_\epsilon$
\[ 2^{-n\left(H_\pi  + \epsilon\right)} \leq \pi(\boldsymbol X ) \leq 2^{-n\left(H_\pi -\epsilon\right)} \]
holds, where $H_\pi = \mathbb{E}_\pi[\log_2(x)]$ is the entropy of $\pi(\cdot)$. 
In practice we cannot know for sure whether our chain has explored the typical set.
Even if our algorithm is correctly constructed and our efficiency measures suggest satisfactory sampling, it might be the case that our chain is stuck at a mode, for instance, and not sampling the whole space.
For most target distributions of interest in practice, the issue of determining the typical set remains an open problem.
See chapter 3 for a first -- and informal --  stab at characterising the typical set for time-calibrated phylogenies.

Contrary to what is routinely suggested in the literature, MCMC is not a Bayesian method.
Rather, MCMC is a computational method for approximating integrals and is completely agnostic about what is being computed.
As a counter-example to the claim the MCMC is ``Bayesian'' in any way, I offer~\cite{Geyer1991} and~\cite{Kuhner1998}, who employ MCMC for maximum likelihood estimation\footnote{See also~\url{http://users.stat.umn.edu/~geyer/mcmc/diag.html} -- also cited in Chapter 3 -- for the precious quote: ``[MCMC] isn't even statistics, it is a tool.''. Terms such as ``Bayesian MCMC'' make no sense whatsoever. }.
Moreover, MCMC is not the only tool to compute integrals and approximate distributions; particle filtering -- also called sequential Monte Carlo --\citep{Gordon1993, DelMoral1996} --, rejection sampling~\citep{Casella2004} and importance sampling~\citep{Rubinstein2016} are examples of techniques that do not involve the construction of Markov chains.

\section{Software}
\label{sec:software}

In the course of my doctoral work I have written a fair amount of computer programs (scripts) to perform several custom analyses as well as to automate data wrangling and processing tasks.
In this short section I detail some of the software I have used in my research and acknowledge the efforts of the many programmers who developed these tools without which my work would not have been possible.
The first point to notice is that most (if not all) software I have used was open source software, that is, software for which the source code is released under a licence that allows the end user to study, change, and distribute the software to anyone and for any purpose~\citep{Laurent2004}.
This is important insofar as it allows one to see what is ``under the hood'', the inner workings of the software under use, a crucial aspect of ensuring scientific correctness~\citep{Darriba2018}.
Throughout my PhD I have used a GNU/Linux desktop (and server) system, which sports a host of useful tools that were crucial for my research.
The first of these tools is the bash shell (from UNIX), which allowed me to script many otherwise tedious and error-prone tasks.
A second tool that I made extensive use of was \verb|GNU Parallel|~\citep{Tange2011}, which allows executing jobs in parallel and hence exploit multi-core architectures in most modern personal computers and servers.

Most of the programs I wrote for my research were written in the R statistical computing language~\citep{R2017}, using the open source integrated development environment (IDE) Rstudio~\citep{Rstudio2015}.
R offers an enormous variety of user-contributed packages which greatly expand the capabilities of the base distribution. 
I have used the \textbf{ape}~\citep{Paradis2004},~\textbf{parallel}~\citep{R2017},~\textbf{ggplot2}~\citep{Wickham2009},~\textbf{viridis}~\citep{Garnier2018} and~\textbf{data.table}~\citep{Dowle2017} packages repeatedly across several projects.
This list is by no means exhaustive; please see the individual chapters for more details on the R packages (and other software) I have used in each of the projects.
BEAST (see below) is written in the cross-platform JAVA language (\url{https://java.com/}) and I have used the Eclipse IDE (\url{https://www.eclipse.org/ide/}) to aid both code reading and minor development.
Finally, I would like to mention that I have made extensive usage of the excellent version-control system \verb|git| and the associated code hosting service GitHub (\url{https://github.com/}).
This thesis was written using the open-source typesetting language \LaTeX\: and its source code is hosted at~\url{https://github.com/maxbiostat/PhD_Thesis}.

\subsubsection{BEAST}
\label{sec:beast}

The software package BEAST, short for \textbf{B}ayesian \textbf{E}volutionary \textbf{A}nalysis by \textbf{S}ampling \textbf{T}rees~\citep{Drummond2007,Drummond2012,Suchard2018} is a central part of this thesis.
BEAST is specially designed for the estimation of rooted phylogenies from genetic sequences, with a heavy focus on time-calibrated phylogenies.
It offers a large number of tree priors (coalescent- and non coalescent-based models) as well as specialised computational machinery for the construction and estimation of discrete and continuous phylogeographic models~\citep{Lemey2009,Lemey2010,Pybus2012,Dudas2017}.
The interested reader is referred to the on-line documentation (\url{http://beast.community/}) for more information.

With regard to MCMC, BEAST uses Metropolis-Hastings as its main algorithm, while specialised samplers do exist for some model components.
For example  a Gibbs sampler is used to efficiently sample the quantities in the Skygrid model described above.
In BEAST~\textit{parlance}, transition kernels are called~\textbf{operators} and I shall employ the terms interchangeably. 
BEAST employs adaptive MCMC, meaning that it tunes the scale of its operators to achieve a target acceptance probability -- currently $0.234$ for most operators.
It is important to notice, however, that not all operators in BEAST can be adjusted in this way, and are therefore called ``non-tunable'', in contrast to the ``tunable'' ones.

The transition kernels described in Chapter 2 were implemented in BEAST\footnote{I would like to explicitly acknowledge Andrew Rambaut's extensive help with understanding BEAST and its inner workings, as well as implementing many of the ideas explored in this thesis, for which I am very grateful.} and the visualisations in Chapter 3 would not have been possible without specialised classes in the BEAST code base (see Section~\ref{sec:accommodating} therein).
Moreover, BEAST output was also important in the analyses I present in Chapters 4 and 5, where I sought to combine phylodynamic data from BEAST analyses with epidemiological information.

\section{Goals}
\label{sec:goals}

Since its emergence in the later half of the 2000s, phylodynamics has grown into a powerful tool in the study of pathogen dynamics.
However, as many authors~\citep{Volz2013,Pybus2013} note, there is an ever growing gap between data accumulation and the methodological apparatus to analyse and integrate this data.

This PhD thesis is an attempt at plugging that gap, and has two main axes: (a) the development of more efficient transition kernels and visualisations for MCMC in phylogenetic space and (b) the application of state-of-the-art statistical methods to address phylodynamic/epidemiological questions.
In chapters 2 and 3 I outline a proposal for new transition kernels (operators) and address the modification of existing convergence diagnostics to large, time-calibrated phylogenies.
Chapters 4 and 5 showcase how one can combine modern statistical tools and phylogenetic/phylodynamic data to explore the evolutionary and epidemiological dynamics of pathogens such as Ebola virus.
In chapter 6 I discuss the overall impact of the findings in this thesis and how I see the field moving forward.

% \bibliography{/home/max/Dropbox/PHD/THESIS/bibliography/lmcarvalho_PhD_Thesis}